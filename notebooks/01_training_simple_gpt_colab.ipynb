{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro"
   },
   "source": [
    "# Training a Simple GPT Model (Colab Version)\n\nIn this notebook, we will train a small GPT model on the Daily Dialog dataset using Hugging Face transformers library. This version is specifically adapted for Google Colab and includes all necessary code to train the model.\n\nFirst, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "install_packages"
   },
   "source": [
    "!pip install transformers datasets torch tqdm matplotlib"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\n# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_section"
   },
   "source": [
    "## Model Configuration\n\nFirst, let's define our model configuration. Feel free to modify these parameters to experiment with different model sizes and training settings."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "config_implementation"
   },
   "source": [
    "class Config:\n    # Model architecture parameters\n    n_layer = 6          # Number of transformer layers\n    n_head = 8           # Number of attention heads\n    n_embd = 512        # Embedding dimension\n    block_size = 128     # Maximum sequence length\n    dropout = 0.1        # Dropout rate\n    \n    # Training parameters\n    batch_size = 32\n    learning_rate = 3e-4\n    max_iters = 5000\n    eval_interval = 500   # How often to evaluate on validation set\n    eval_iters = 200     # How many batches to evaluate on\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Initialize configuration\nconfig = Config()\n\n# Print configuration\nprint(f\"Model Configuration:\")\nprint(f\"- Number of layers: {config.n_layer}\")\nprint(f\"- Number of attention heads: {config.n_head}\")\nprint(f\"- Embedding dimension: {config.n_embd}\")\nprint(f\"- Maximum sequence length: {config.block_size}\")\nprint(f\"- Dropout rate: {config.dropout}\")\nprint(f\"\\nTraining Configuration:\")\nprint(f\"- Batch size: {config.batch_size}\")\nprint(f\"- Learning rate: {config.learning_rate}\")\nprint(f\"- Maximum iterations: {config.max_iters}\")\nprint(f\"- Device: {config.device}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_section"
   },
   "source": [
    "## Data Loading and Preprocessing\n\nNow we'll load and preprocess the Daily Dialog dataset using Hugging Face's datasets library."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "data_implementation"
   },
   "source": [
    "def load_and_preprocess_data(config):\n    \"\"\"Load and preprocess the Daily Dialog dataset using HuggingFace.\"\"\"\n    # Load the dataset\n    print(\"Loading dataset...\")\n    dataset = load_dataset(\"daily_dialog\")\n    \n    # Initialize tokenizer\n    print(\"Initializing tokenizer...\")\n    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    def tokenize_function(examples):\n        \"\"\"Tokenize a batch of examples.\"\"\"\n        # Join all dialogues with spaces and add EOS token\n        texts = [\" \".join(d) + tokenizer.eos_token for d in examples[\"dialog\"]]\n        return tokenizer(\n            texts,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=config.block_size,\n            return_tensors=\"pt\"\n        )\n    \n    # Process training data\n    print(\"Processing training data...\")\n    train_data = dataset[\"train\"].map(\n        tokenize_function,\n        batched=True,\n        batch_size=100,\n        remove_columns=dataset[\"train\"].column_names,\n        num_proc=4\n    )\n    \n    # Process validation data\n    print(\"Processing validation data...\")\n    val_data = dataset[\"validation\"].map(\n        tokenize_function,\n        batched=True,\n        batch_size=100,\n        remove_columns=dataset[\"validation\"].column_names,\n        num_proc=4\n    )\n    \n    # Convert to PyTorch datasets\n    train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n    val_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n    \n    return train_data, val_data, tokenizer\n\ndef get_batch(split, train_data, val_data, config):\n    \"\"\"Get a batch of data for training or validation.\"\"\"\n    data = train_data if split == 'train' else val_data\n    \n    # Generate random indices\n    ix = torch.randint(len(data), (config.batch_size,))\n    \n    # Get the sequences\n    batch = data[ix]\n    x = batch[\"input_ids\"]\n    \n    # Create input-target pairs by shifting\n    x = x[:, :-1]\n    y = batch[\"input_ids\"][:, 1:]\n    \n    # Move to appropriate device\n    x, y = x.to(config.device), y.to(config.device)\n    \n    return x, y\n\n# Load and preprocess data\ntrain_data, val_data, tokenizer = load_and_preprocess_data(config)\n\nprint(f\"\\nDataset Statistics:\")\nprint(f\"- Training examples: {len(train_data)}\")\nprint(f\"- Validation examples: {len(val_data)}\")\nprint(f\"- Vocabulary size: {len(tokenizer)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_section"
   },
   "source": [
    "## Model Implementation\n\nLet's implement our GPT model using HuggingFace's transformers library."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "model_implementation"
   },
   "source": [
    "class GPT(nn.Module):\n    \"\"\"GPT Language Model using HuggingFace's GPT2\"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        self.config = GPT2Config(\n            n_layer=config.n_layer,\n            n_head=config.n_head,\n            n_embd=config.n_embd,\n            vocab_size=50257,  # GPT-2 vocabulary size\n            n_positions=config.block_size,\n            n_ctx=config.block_size,\n            dropout=config.dropout\n        )\n        self.transformer = GPT2LMHeadModel(self.config)\n        self.block_size = config.block_size\n\n    def forward(self, idx, targets=None):\n        # Forward pass through GPT-2\n        outputs = self.transformer(\n            idx,\n            labels=targets if targets is not None else None,\n            return_dict=True\n        )\n        \n        if targets is not None:\n            return outputs.logits, outputs.loss\n        else:\n            return outputs.logits, None\n\n@torch.no_grad()\ndef estimate_loss(model, train_data, val_data, config):\n    \"\"\"Estimate loss on train and validation sets\"\"\"\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(config.eval_iters)\n        for k in range(config.eval_iters):\n            X, Y = get_batch(split, train_data, val_data, config)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n@torch.no_grad()\ndef generate(model, tokenizer, prompt, max_tokens=50, temperature=1.0, top_k=None, config=None):\n    \"\"\"Generate text from a prompt.\"\"\"\n    # Encode the prompt\n    encoded = tokenizer.encode(prompt, return_tensors='pt').to(config.device)\n    \n    # Generate using HuggingFace's built-in generation\n    output_sequences = model.transformer.generate(\n        encoded,\n        max_length=len(encoded[0]) + max_tokens,\n        temperature=temperature,\n        top_k=top_k if top_k is not None else 50,\n        top_p=0.9,\n        do_sample=True,\n        pad_token_id=tokenizer.pad_token_id,\n        bos_token_id=tokenizer.bos_token_id,\n        eos_token_id=tokenizer.eos_token_id\n    )\n    \n    # Decode the generated text\n    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n    return generated_text\n\n# Initialize the model\nmodel = GPT(config)\nmodel.to(config.device)\n\n# Print model summary\nprint(\"Model Architecture:\")\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"- Total parameters: {total_params:,}\")\nprint(f\"- Trainable parameters: {trainable_params:,}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_section"
   },
   "source": [
    "## Training Loop\n\nNow we'll train the model, tracking both training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "training_implementation"
   },
   "source": [
    "# Create optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n\n# Lists to store losses for plotting\ntrain_losses = []\nval_losses = []\niterations = []\n\n# Training loop\nfor iter in tqdm(range(config.max_iters), desc=\"Training\"):\n    # Sample a batch of data\n    xb, yb = get_batch('train', train_data, val_data, config)\n    \n    # Forward pass\n    logits, loss = model(xb, yb)\n    \n    # Backward pass\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n    \n    # Evaluate the model\n    if iter % config.eval_interval == 0:\n        losses = estimate_loss(model, train_data, val_data, config)\n        print(f\"Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n        \n        # Store losses for plotting\n        train_losses.append(losses['train'])\n        val_losses.append(losses['val'])\n        iterations.append(iter)\n\n# Plot learning curves\nplt.figure(figsize=(10, 6))\nplt.plot(iterations, train_losses, label='Train Loss')\nplt.plot(iterations, val_losses, label='Validation Loss')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.grid(True)\nplt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generation_section"
   },
   "source": [
    "## Text Generation\n\nLet's test our trained model by generating some text:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "generation_implementation"
   },
   "source": [
    "# Test text generation with different prompts and temperatures\nprompts = [\n    \"Hello, how are you\",\n    \"I'm planning to\",\n    \"The weather is\"\n]\n\ntemperatures = [0.7, 1.0, 1.2]\n\nfor prompt in prompts:\n    print(f\"\\nPrompt: {prompt}\")\n    for temp in temperatures:\n        generated = generate(model, tokenizer, prompt, max_tokens=50, temperature=temp, config=config)\n        print(f\"\\nTemperature {temp}:\")\n        print(generated)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_section"
   },
   "source": [
    "## Save the Model\n\nFinally, let's save our trained model and download it from Colab:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "save_implementation"
   },
   "source": [
    "# Create a directory for the model\nimport os\nos.makedirs('models', exist_ok=True)\n\n# Save the model\nmodel_path = 'models/gpt_model'\nmodel.transformer.save_pretrained(model_path)\ntokenizer.save_pretrained(model_path)\n\nprint(f\"Model and tokenizer saved to {model_path}\")\n\n# For Colab: Download the model\nfrom google.colab import files\n!zip -r models.zip models/\nfiles.download('models.zip')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Training a Simple GPT Model",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}